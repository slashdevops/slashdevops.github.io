[{"content":"Introduction I have been using WordPress for a long time to host slashdevops.com blog site. I had been looking for a way to migrate this blog from WordPress to a static site generator and during the research I found Hugo and I can say I am very happy with it. Hugo allows me to write this blog posts in Markdown and it is very fast and easy to use.\nI have also been using GitHub Pages for other of my personal jobs and I decided to migrate this blog GitHub Pages as well.\nTo perform this migration I used the following software:\nHugo GitHub Pages Google Domains Disqus Fuse.js Hugo Theme -\u0026gt; PaperMod, Thanks to Aditya Telange for this theme üôè. My Experience It took me some time to migrate this blog from WordPress to Hugo and GitHub Pages. I had to learn how to use Hugo, GitHub Pages and how to configure my slashdevops.com domain in Google Domains but it was worth it. I lost some of the WordPress features like comments and likes. Now, the blog implement Disqus for comments, and I am trying to find a way to implement likes.\nThe most hard part was understand how should work the GitHub Pages and Hugo together. There are several tutorials on the internet that can help you with this migration, but not even the official documentation of Hugo and GitHub Pages helped me to understand the fact that you need at least two repositories to have this done. One repository to host the source code of the blog and another repository to host the compiled code of the blog (static files from public folder of Hugo).\nThe source code repository is the one you use to write your blog posts and the compiled code repository is the one you use to host the blog.\nThe Good üëç Hugo is very fast and easy to use. Hugo allows me to write this blog posts in Markdown. Hugo has a lot of themes to choose from. GitHub Pages is free and easy to use. The blog implement fuzzy search using Fuse.js. The Bad üëé Migrating from WordPress to Hugo was not easy. I made it manually and it took me some time, fortunately I didn\u0026rsquo;t have many posts. At the beginning, I had some issues with the Hugo theme I chose, but I was able to fix them. I had to learn how to use Hugo and GitHub Pages but it was worth it. I lost some of the WordPress features like comments and likes but I am happy with the result. This blog implement Disqus for comments, and I am trying to find a way to implement likes. I lost the comments and likes from the WordPress posts, I\u0026rsquo;m very sorry about that. Not so Bad not so Good ü´§ To have GitHub Pages for free, I had to use a public repository -\u0026gt; slashdevops.github.io repository, but I am happy with it. This means you are seeing the source code of this blog and how it is built. Currently, I am using Disqus for comments, but I am trying to find a way to implement likes. Relevant steps The most relevant information about this migration is and I like to share with you is:\n1. Repository Configuration As I explained before, to have this blog working properly you need at least two repositories. So, for my migration I used the following repositories:\n1.1 slashdevops.github.io repository This repository slashdevops/slashdevops.github.io is used to host the compiled code of the blog (static files coming from Hugo folder public). This should be filled using the gihub actions workflow to build the blog and push the static files to this repository. This should be public to enable the github pages feature for free.\nThis repository could not be empty to enable the github pages feature. So, I created a index.html file with a minimal content to avoid the repository being empty.\nThe file I added to the repository slashdevops/slashdevops.github.io is:\n1 2 3 4 5 6 7 8 9 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Home\u0026lt;/title\u0026gt; \u0026lt;/html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to the home page!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This repository configuration looks like this:\nand after push the minimal html file to the repository, and configure your domain DNS as it is explained in the step 2 below, you can see the blog site working properly.\n1.2 hugo-slashdevops.github.io repository This repository slashdevops/hugo-slashdevops.github.io is used to host the source code of the blog. Basically, this repository contains the Hugo configuration, the Markdown files of the blog posts and the Github Action Workflow to build and deploy the static files. Also, this repository should be private to avoid the source code of the blog being exposed.\nFor the this repository, the hugo configuration is very important to have the blog working properly.\n2. Google Domains Configuration For this configuration, I made the validation of the domain slashdevops.com my github settings and then I configured the domain in Google Domains to point to the GitHub Pages servers.\nThis image shows how I configure the domain in GitHub:\nand this other how looks the domain after the configuration:\nThen, I configured the domain in Google Domains to point to the GitHub Pages servers. This configuration is done in the DNS section of the domain configuration in Google Domains.\nThis image shows how I configured the domain in Google Domains:\nReferences:\nhttps://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site#configuring-an-apex-domain 3. Hugo Configuration The file Hugo.toml I used to build this blog.\n4. GitHub Workflow The GitHub Workflow -\u0026gt; .github/workflows/hugo.yml\n5. Disqus Configuration The Disqus Configuration I used to implement comments in this blog.\n6. .gitignore configuration The .gitignore configuration I used to ignore some files in this blog.\n7. CNAME configuration The CNAME configuration I used to configure the domain of this blog. Ensure this file contains the domain you want to use.\nReferences:\nhttps://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/troubleshooting-custom-domains-and-github-pages#cname-errors ","permalink":"https://slashdevops.com/post/2024/03/30/1/migrated-from-wordpress-to-hugo-and-github-pages/","summary":"Introduction I have been using WordPress for a long time to host slashdevops.com blog site. I had been looking for a way to migrate this blog from WordPress to a static site generator and during the research I found Hugo and I can say I am very happy with it. Hugo allows me to write this blog posts in Markdown and it is very fast and easy to use.\nI have also been using GitHub Pages for other of my personal jobs and I decided to migrate this blog GitHub Pages as well.","title":"Migrated from WordPress to Hugo and GitHub Pages"},{"content":"The Problem Surely and like me, you are trying to be more secure when connecting Jenkins with your AWS Accounts assuming a role. If you are asking What is that? , please read this: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nOf course, there are many different options to use, but the problem always surrounds us, if you use a plugin then the maintainability and security when talking about Jenkins plugins for sure decrease.\nI particularly hate Jenkins, from my point of view this is an obsolete tool trying to survive in the modern world, and if you are concerned about security (and maintainability) sure understand my point.\nSo, why I‚Äôm writing about that? Because unfortunately I still using Jenkins and sweating their maintenance Because as a rule of thumb I try to avoid plugins that don‚Äôt have any release in the time windows of 6-12 months Helps others to avoid loose time and security when needs the same that me, AWS cross-account connections using Jenkins assuming a role Because at least if I have an Issue this is my code and I can fix it What‚Äôs this? This is a guide and code for somebody using Jenkins shared library This is a minimal blog entry to help someone that understands Jenkins and Groovy This could help you if you are using Jenkins + Jenkins shared library + AWS cross-account and cross-region roles what it is not? A tutorial A very well-explained and step-by-step guide Something you surely need to use An AWS cross-account tutorial or explanation guide The Solution This is how looks a segment of the code on my production Jenkins declarative pipeline.\nLook at withAwsEnVars (lines: 4, 11) pipeline tags\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ... stage(\u0026#39;setup repositories\u0026#39;) { steps { withAwsEnVars(roleName: cicd.getRole(), roleAccount: codeArtifact.getOwner(\u0026#39;npm-private\u0026#39;)) { script { log.info(\u0026#39;setting repositories \\\u0026#39;npm-private\\\u0026#39; credentials for dependencies\u0026#39;) codeArtifact.setupNpmrc(\u0026#39;npm-private\u0026#39;, \u0026#39;@my-company-namespace\u0026#39;, params.timeoutTime*60) codeArtifact.setupNpmrc(\u0026#39;npm-private\u0026#39;, \u0026#39;@my-company-other-namespace\u0026#39;, params.timeoutTime*60) } } withAwsEnVars(roleName: cicd.getRole(), roleAccount: codeArtifact.getOwner(\u0026#39;npm-public\u0026#39;)) { script { log.info(\u0026#39;setting repositories \\\u0026#39;npm-public\\\u0026#39; credentials for dependencies\u0026#39;) codeArtifact.setupNpmrc(\u0026#39;npm-public\u0026#39;, \u0026#39;\u0026#39;, params.timeoutTime*60) } } } } ... withAwsEnVars is a Groovy function used in my Jenkins Shared Library and this is the withAwsEnVars.groovy file content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 #!/usr/bin/env groovy /* paramters: roleArn (required) roleAccount (required) sessionName (optional) sessionDuration (optional) Examples: stage(\u0026#39;test aws credential\u0026#39;) { steps { withAwsEnVars(roleName:\u0026#39;cicd-execution-role\u0026#39;, roleAccount: \u0026#39;12345678910\u0026#39;) { sh \u0026#34;echo TOKEN: ${AWS_SESSION_TOKEN}\u0026#34; sh \u0026#34;echo KEY: ${AWS_SECRET_ACCESS_KEY}\u0026#34; sh \u0026#34;echo ID: ${AWS_ACCESS_KEY_ID}\u0026#34; sh \u0026#39;aws s3 ls\u0026#39; } sh \u0026#34;exit 1\u0026#34; } } */ def call(Map params, Closure body) { if (!params.roleName) { error \u0026#34;\u0026#34;\u0026#34; parameter \u0026#39;roleName\u0026#39; is required. --- Example: withAwsEnVars(roleName:\u0026#39;cicd-execution-role\u0026#39;, roleAccount: \u0026#39;12345678910\u0026#39;) {...} \u0026#34;\u0026#34;\u0026#34; } if (!params.roleAccount) { error \u0026#34;\u0026#34;\u0026#34; parameter \u0026#39;roleAccount\u0026#39; is required. --- Example: withAwsEnVars(roleName:\u0026#39;cicd-execution-role\u0026#39;, roleAccount: \u0026#39;12345678910\u0026#39;) {...} \u0026#34;\u0026#34;\u0026#34; } // get optional parameters if not set default String sessionName = params.get(\u0026#39;sessionName\u0026#39;, \u0026#39;jenkins\u0026#39;) Integer duration = params.get(\u0026#39;sessionDuration\u0026#39;, 900) cred = awsCredentials.getFromAssumeRole(params.roleName, params.roleAccount, sessionName, duration) AWS_ACCESS_KEY_ID = cred.AccessKeyId AWS_SECRET_ACCESS_KEY = cred.SecretAccessKey AWS_SESSION_TOKEN = cred.SessionToken wrap([ $class: \u0026#39;MaskPasswordsBuildWrapper\u0026#39;, varPasswordPairs: [ [password: AWS_ACCESS_KEY_ID], [password: AWS_SECRET_ACCESS_KEY], [password: AWS_SESSION_TOKEN] ] ]) { withEnv([ \u0026#34;AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\u0026#34;, \u0026#34;AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\u0026#34;, \u0026#34;AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}\u0026#34; ]) { body() } } } and this is my awsCredentials.groovy file content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #!/usr/bin/env groovy /* Parameters: roleName (required) roleAccount (required) Examples: cred = awsCredentials.getFromAssumeRole(...) accessKey = awsCredentials.getFromAssumeRole(...).AccessKeyId */ String getFromAssumeRole(String roleName, String roleAccount, String sessionName=\u0026#39;jenkins\u0026#39;, Integer duration=900){ String roleArn = \u0026#39;arn:aws:iam::\u0026#39; + roleAccount +\u0026#39;:role/\u0026#39;+ roleName List\u0026lt;String\u0026gt; options = [] options += \u0026#34;--role-arn ${roleArn}\u0026#34; options += \u0026#34;--role-session-name ${sessionName}\u0026#34; options += \u0026#34;--duration-seconds ${duration}\u0026#34; options += \u0026#34;--query \u0026#39;Credentials\u0026#39;\u0026#34; optionsString = options.join(\u0026#34; \u0026#34;) // this is used to mask any critical information wrap([$class: \u0026#39;MaskPasswordsBuildWrapper\u0026#39;, varPasswordPairs: [[password: roleArn], [password: sessionName]]]) { String strCreds = sh( returnStdout: true, script: \u0026#34;\u0026#34;\u0026#34; aws sts assume-role ${optionsString} \u0026#34;\u0026#34;\u0026#34;).trim() return readJSON(text: strCreds) } } from the code above, definitions are located in the Jenkins Shared Library\ncodeArtifact.setupNpmrc(\u0026hellip;) -\u0026gt; codeArtifact.groovy codeArtifact.getOwner(\u0026hellip;) -\u0026gt; codeArtifact.groovy cicd.getRole() -\u0026gt; cicd.groovy The minimal requirements on your Jenkins controller and agents AWS CLI Jenkins Plugin ‚Äì Pipeline Utility Steps ‚Äì\u0026gt; How to use? Jenkins Plugin ‚Äì Pipeline: Basic Steps ‚Äì\u0026gt;How to use? Jenkins Plugin ‚Äì Mask Passwords ‚Äì\u0026gt; How to use? So, What is the Magic? Why do I say this is secure? Maybe after looking at the following pipeline code, you will see how easy is to use this, and this is secure because if you execute the following code in your pipeline:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... stage(\u0026#39;test aws credential\u0026#39;) { steps { withAwsEnVars(roleName:\u0026#39;cicd-execution-role\u0026#39;, roleAccount: \u0026#39;12345678910\u0026#39;) { sh \u0026#34;echo TOKEN: ${AWS_SESSION_TOKEN}\u0026#34; sh \u0026#34;echo KEY: ${AWS_SECRET_ACCESS_KEY}\u0026#34; sh \u0026#34;echo ID: ${AWS_ACCESS_KEY_ID}\u0026#34; sh \u0026#39;aws s3 ls\u0026#39; } sh \u0026#34;exit 1\u0026#34; } } ... you will see masked the TOKEN, KEY and ID. Instead of seeing the real value, you will see *********** characters.\nTools and Concepts There are various tools and concepts I used here, the first that allows me to do that so easily was Groovy Closures and it is explained how to use on Jenkins Shared Library ‚Äì\u0026gt; Defining custom steps.\nThen we have the tool withEnv provided by the Jenkins Plugin ‚Äì Pipeline: Basic Steps and in combination with the use of the Groovy Closures allowed me to export the AWS Environment Variables coming from awsCredentials.getFromAssumeRole(‚Ä¶) groovy function into a container script part.\nBut, make sure that anyone who will use Jenkins Controller and Pipeline doesn‚Äôt have access to the values of the AWS Environment Variables is the job of wrap provided by the Jenkins Plugin ‚Äì Pipeline: Basic Steps + maskPasswords provided by Jenkins Plugin ‚Äì Mask Passwords.\nOthers Links https://www.jenkins.io/doc/book/installing/ https://www.jenkins.io/doc/book/using/ https://www.jenkins.io/doc/book/security/#securing-jenkins https://www.jenkins.io/doc/book/pipeline/ Closing Even hating Jenkins like me, you can find different ways to do your life easy and secure with him.\nIf you want to look at my GitHub repositories related to Jenkins, here you have:\nhttps://github.com/christiangda/jenkins-casc-controller https://github.com/christiangda/jenkins-shared-library ","permalink":"https://slashdevops.com/post/2022/12/03/1/secure-and-easy-aws-connection-assuming-a-role-with-jenkins-shared-library/","summary":"The Problem Surely and like me, you are trying to be more secure when connecting Jenkins with your AWS Accounts assuming a role. If you are asking What is that? , please read this: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nOf course, there are many different options to use, but the problem always surrounds us, if you use a plugin then the maintainability and security when talking about Jenkins plugins for sure decrease.\nI particularly hate Jenkins, from my point of view this is an obsolete tool trying to survive in the modern world, and if you are concerned about security (and maintainability) sure understand my point.","title":"Secure and Easy AWS Connection Assuming a Role With Jenkins Shared Library"},{"content":"Operating System Install Rosetta 1 /usr/sbin/softwareupdate --install-rosetta --agree-to-license Install Xcode 1 xcode-select --install Package Manager Install Homebrew 1 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Add Homebrew to your PATH -\u0026gt; /Users/\u0026lt;user home\u0026gt;/.zprofile 1 2 echo \u0026#39;eval $(/opt/homebrew/bin/brew shellenv)\u0026#39; \u0026gt;\u0026gt; /Users/$USER/.zprofile eval $(/opt/homebrew/bin/brew shellenv) (OPTIONAL) Update/upgrade Homebrew 1 brew update \u0026amp;\u0026amp; brew upgrade Terminal and Mods Install iterm2 1 brew install --cask iterm2 WARNING: After this step close the default term and open iterm2\nInstall Oh My Zsh 1 sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; (OPTIONAL) Update Oh My Zsh 1 omz update Install Oh My Zsh useful plugins List available plugins on your local installation 1 ls ~/.oh-my-zsh/plugins Configure my plugins NOTE: my useful plugins list is in the MY_PLUGINS_LIST env var, so check it for yours\n1 2 3 4 # check it to add or remove yours export MY_PLUGINS_LIST=\u0026#34;git aws golang zsh-navigation-tools brew docker docker-compose minikube kubectl ansible virtualenv python rust terraform vscode podman\u0026#34; sed -i\u0026#34;bkup\u0026#34; \u0026#34;s/plugins\\=(git)/plugins\\=($MY_PLUGINS_LIST)/\u0026#34; ~/.zshrc Install and configure Custom Plugins 1 2 3 4 5 6 7 8 9 10 11 # zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting sed -i\u0026#34;bkup\u0026#34; \u0026#39;/plugins\\=/ s/)$/ zsh-syntax-highlighting)/\u0026#39; ~/.zshrc # zsh-autosuggestions git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions sed -i\u0026#34;bkup\u0026#34; \u0026#39;/plugins\\=/ s/)$/ zsh-autosuggestions)/\u0026#39; ~/.zshrc # zsh-completions git clone https://github.com/zsh-users/zsh-completions ${ZSH_CUSTOM:=~/.oh-my-zsh/custom}/plugins/zsh-completions sed -i\u0026#34;bkup\u0026#34; \u0026#39;/plugins\\=/ s/)$/ zsh-completions)/\u0026#39; ~/.zshrc (OPTIONAL) Configure and install Powerlevel10k Theme for Zsh Install Powerlevel10 on Oh My Zsh 1 git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k (OPTIONAL) Update Powerlevel10k 1 git -C ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k pull Configure Oh My Zsh to use Powelevel10k 1 sed -i\u0026#34;bkup\u0026#34; \u0026#39;s/ZSH_THEME\\=\\\u0026#34;robbyrussell\\\u0026#34;/ZSH_THEME\\=\\\u0026#34;powerlevel10k\\/powerlevel10k\\\u0026#34;/\u0026#39; ~/.zshrc (OPTIONAL) Configure Powelevel10k to show only the last directory 1 typeset -g POWERLEVEL9K_SHORTEN_STRATEGY=truncate_to_last WARNING:\nAfter this step close the iterm2 console Once the iterm2 opens, the process of customization starts, and then closes the iterm2 again to begin the process of configuration of Powerlevel10k Configure Zsh history Add history configuration to file ~/.zshrc appending it\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt;_EOL_ # History HISTFILE=\u0026#34;\\$HOME/.zsh_history\u0026#34; HISTSIZE=500000 SAVEHIST=500000 setopt BANG_HIST # Treat the \u0026#39;!\u0026#39; character specially during expansion. setopt EXTENDED_HISTORY # Write the history file in the \u0026#34;:start:elapsed;command\u0026#34; format. setopt INC_APPEND_HISTORY # Write to the history file immediately, not when the shell exits. setopt SHARE_HISTORY # Share history between all sessions. setopt HIST_EXPIRE_DUPS_FIRST # Expire duplicate entries first when trimming history. setopt HIST_IGNORE_DUPS # Don\u0026#39;t record an entry that was just recorded again. setopt HIST_IGNORE_ALL_DUPS # Delete old recorded entry if new entry is a duplicate. setopt HIST_FIND_NO_DUPS # Do not display a line previously found. setopt HIST_IGNORE_SPACE # Don\u0026#39;t record an entry starting with a space. setopt HIST_SAVE_NO_DUPS # Don\u0026#39;t write duplicate entries in the history file. setopt HIST_REDUCE_BLANKS # Remove superfluous blanks before recording entry. setopt HIST_VERIFY # Don\u0026#39;t execute immediately upon history expansion. setopt HIST_BEEP # Beep when accessing nonexistent history. # End of History _EOL_ Apply changes\n1 source ~/.zshrc IDE and Plugins Install Visual Studio Code Using brew cask\n1 brew install --cask visual-studio-code Add vscode to the PATH useful to call vscode from anywhere\n1 2 3 4 cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ~/.zprofile # Add Visual Studio Code (code) export PATH=\u0026#34;\\$PATH:/Applications/Visual Studio Code.app/Contents/Resources/app/bin\u0026#34; EOF Reload the terminal 1 source ~/.zprofile Install my default extensions 1 2 3 4 code --install-extension eamodio.gitlens code --install-extension streetsidesoftware.code-spell-checker code --install-extension yzhang.markdown-all-in-one code --install-extension redhat.vscode-yaml Container Management Install Podman Like docker but better because this is free and open source.\nUsing brew cask\n1 brew install podman Install podman-desktop 1 brew install podman-desktop Initialize podman machine This is the container we have in macos to use podman like docker does.\n1 podman machine init NOTE: good reference here https://docs.podman.io/en/latest/markdown/podman-machine-init.1.html\nStart podman machine 1 podman machine start NOTE:\ngood reference here https://docs.podman.io/en/latest/markdown/podman-machine-start.1.html podman could wrapper docker, so if you don‚Äôt have docker installed just created a terminal alias in your OS. (OPTIONAL) Create a docker alias to podman This is in case you don‚Äôt have installed docker and you want to use podman as a wrapper of this:\n1 2 3 4 5 cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ~/.zshrc # docker alias to podman # remove it if you want to install docker alias docker=podman EOF ","permalink":"https://slashdevops.com/post/2022/09/17/1/my-custom-macbook-ro-m1-m2-provisioning/","summary":"Operating System Install Rosetta 1 /usr/sbin/softwareupdate --install-rosetta --agree-to-license Install Xcode 1 xcode-select --install Package Manager Install Homebrew 1 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Add Homebrew to your PATH -\u0026gt; /Users/\u0026lt;user home\u0026gt;/.zprofile 1 2 echo \u0026#39;eval $(/opt/homebrew/bin/brew shellenv)\u0026#39; \u0026gt;\u0026gt; /Users/$USER/.zprofile eval $(/opt/homebrew/bin/brew shellenv) (OPTIONAL) Update/upgrade Homebrew 1 brew update \u0026amp;\u0026amp; brew upgrade Terminal and Mods Install iterm2 1 brew install --cask iterm2 WARNING: After this step close the default term and open iterm2","title":"My custom MacBook Pro [m1|m2] Provisioning"},{"content":"Managing the Lifecycle of your Elasticsearch Indices Just like me, you are probably storing your [Applications | Infrastructure | IoT ] Logs / Traces (as a time series) into Elasticsearch or at least considering doing it.\nIf that is the case, you might be wondering how to efficiently manage index lifecycles in an automated and clean manner, then this post is for you!\nWhat\u0026rsquo;s happening? Basically, this means that your log management/aggregator applications are storing the logs in Elasticsearch using the timestamp (of capture, processing, or another one) for every record of data and grouping, using a pattern for every group.\nIn Elasticsearch terms, this group of logs is called index and the pattern is referring commonly to the suffix used when you create the index name, e.g.: sample-logs-2020-04-25.\nThe problem Until here everything is ok, right? so the problems begin when your data starts accumulating and you don\u0026rsquo;t want to spend too much time/money to store/maintain/delete it.\nAdditionally, you may be managing all the indices the same, regardless of data retention requirements or access patterns. All the indices have the same number of replicas, shards, disk type, etc. In my case, it is more important the first week of indices than the indices three months old.\nAs I mentioned before, depending on your index name and configuration, you will end up with different indexes aggregating logs based on different timeframes.\n1 2 3 4 5 6 ... sample-logs-2020-04-22 sample-logs-2020-04-23 sample-logs-2020-04-24 ... sample-logs-2020-04-27 It is likely that just like I was doing few months back, you are using your custom Script/Application implementing the Elasticsearch Curator API or going directly over the Elasticsearch index API to delete or maintain your indices lifecycle or worst, you are storing your indices forever without any kind of control or deleting it manually.\nMy logs cases Case 1 Third-party applications that use their own index name pattern like indexname-yyyy-mm-dd and I cannot / I don\u0026rsquo;t want to change it.¬†e.g. Zipkin¬†(zipkin-2020-04-25)\nCase 2 My own log aggregator (custom AWS Lambda function) and/or third-party applications like fluentd, LogStash, etc. That allows me to change the index name pattern. So, in this case I can decide how to aggregate my logs and the index pattern name I want to use.\nOne more thing before moving on: The term used for Elasticsearch to create a index per day, hour, month, etc. is rollover.\nThe Solution Preliminaries In my case, I‚Äôm using AWS Elasticsearch Service which is a ‚Äúlittle bit different‚Äù from the Elasticsearch Elastic since AWS decided to create their own Elasticsearch fork called Open Distro for Elasticsearch.\nthe key terms to understand ‚ÄúIndex Lifecycle‚Äù in every Elasticsearch distribution is:\nIndex State Management (ISM) ‚Üí Open Distro for Elasticsearch Index lifecycle management (ILM) ‚Üí Elasticsearch Elastic ElasticSearch concepts are out of the scope of this post, in the below cases I will explain how Open Distro for Elasticsearch manages its indices lifecycle.\nCase 1 \u0026hellip; Remember above.\nThe log management/aggregation application makes the ‚Äúrollover‚Äù of my indices, but I would like to delete/change those after the index has rolled ‚Äî The most common\nCreate an Index State Management Policy to delete indices based on time and/or size and using an Elasticsearch Templates and Elasticsearch Aliases your Elasticsearch engine can delete your indices periodically.\nAnd yes, the result is very similar to what I was doing with my custom AWS Lambda function in Python using Elasticsearch Curator API\nBut, without the hassle of writing any code, handling connection errors, upgrade my code every time my Elasticsearch was upgraded, credentials, changing the env vars to pass the new indices name, etc.\nNow, thanks to ISM I can use a JSON declarative language to define some rules (policies) and the Elasticsearch engine is in charge of the rest.\nPolicies? ‚Ä¶ imagine you can implement this kind of rules\nKeep ‚Äúmy fresh indices‚Äù open to write for 2 days, then After the 2 first days, closes those indices for write operations and keep them until 13 days more, then 15 days after index creation, delete it forever, end What does this means? well, after learning about the ISM Policies and using Kibana Dev Tool, I created a policy name delete_after_15d following the rules described above, and here you have it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # ISM Policy delete_after_15d PUT _opendistro/_ism/policies/delete_after_15d { \u0026#34;policy\u0026#34;: { \u0026#34;policy_id\u0026#34;: \u0026#34;delete_after_15d\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Maintains the indices open by 2 days, then closes those and delete indices after 15 days\u0026#34;, \u0026#34;default_state\u0026#34;: \u0026#34;ReadWrite\u0026#34;, \u0026#34;schema_version\u0026#34;: 1, \u0026#34;states\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ReadWrite\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;read_write\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;ReadOnly\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;2d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;ReadOnly\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;read_only\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;Delete\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;13d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Delete\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;delete\u0026#34;: {} } ] } ] } } NOTE: Notice the highlighted lines, are these my rules described above?\nThen using the following Elasticsearch Templates, I applied the policy (above), see template line (below) 8, to my indices following a pattern in the line 5\n1 2 3 4 5 6 7 8 9 10 # Template sample-logs to apply the ISM Policy delete_after_15d to new indices PUT _template/sample-logs { \u0026#34;index_patterns\u0026#34;: [ \u0026#34;sample-logs-*\u0026#34; ], \u0026#34;settings\u0026#34;: { \u0026#34;index.opendistro.index_state_management.policy_id\u0026#34;: \u0026#34;delete_after_15d\u0026#34; } } Now what? Is it ready?\nFor the new indices, yes. The indices created after you created this template into your Elasticsearch.\nWhat about the old ones?\nThe indices created before applying the index template. For these we need to change its definition and add the line 5\n1 2 3 4 5 6 7 # Change the oldest indices definition to apply the ISM Policy delete_after_15d PUT sample-logs-2020-*/_settings { \u0026#34;settings\u0026#34;: { \u0026#34;index.opendistro.index_state_management.policy_id\u0026#34;: \u0026#34;delete_after_15d\u0026#34; } } But, how do I complete the tasks you mention before?\nDon‚Äôt worry, keep calm!, here https://github.com/slashdevops/es-lifecycle-ism you have the complete explanation to apply this rule in your own Elasticsearch, also how to test it into an Elasticsearch instance or create it locally with docker-compose.\nCase 2 My Elasticsearch rolls over the indices base on time and/or size and I want to have only one entry point (index) to send my logs ‚Äî I think it is the best one\nThe rules again\nRollover ‚Äúmy fresh indices‚Äù after 1 day, then Close those indices for write operations and keep it until 13 days more, then After 15 days of the index was created, delete it forever, end Well, to do that I create an ISM policy named rollover_1d_delete_after_15 to control the state of my indices and using the rollover action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # ISM Policy rollover_1d_delete_after_15 PUT _opendistro/_ism/policies/rollover_1d_delete_after_15 { \u0026#34;policy\u0026#34;: { \u0026#34;policy_id\u0026#34;: \u0026#34;rollover_1d_delete_after_15\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Rollover every 1d, then closes those and delete indices after 15 days\u0026#34;, \u0026#34;default_state\u0026#34;: \u0026#34;Rollover\u0026#34;, \u0026#34;schema_version\u0026#34;: 1, \u0026#34;states\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Rollover\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;rollover\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;1d\u0026#34; } } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;ReadOnly\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;2d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;ReadOnly\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;read_only\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;Delete\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;13d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Delete\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;delete\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [] } ] } } NOTE: Notice the highlighted lines, did you see the rollover action?\nThen like in case 1, using the following Elasticsearch Templates, I applied the policy above, see template (below) line 7, to my indices following a pattern in the line 5.\n1 2 3 4 5 6 7 8 9 10 11 12 # Template sample-logs-rollover to apply the ISM Policy # rollover_1d_delete_after_15 to new indices PUT _template/sample-logs-rollover { \u0026#34;index_patterns\u0026#34;: [ \u0026#34;sample-logs-rollover-*\u0026#34; ], \u0026#34;settings\u0026#34;: { \u0026#34;index.opendistro.index_state_management.policy_id\u0026#34;: \u0026#34;rollover_1d_delete_after_15\u0026#34;, \u0026#34;index.opendistro.index_state_management.rollover_alias\u0026#34;: \u0026#34;sample-logs-rollover\u0026#34; } } What does it means?\nNow Elasticsearch Engine will be in charge of rollover the indices and you don‚Äôt need to create any index name pattern when indexing your data over Elasticsearch, in other words, your Application logs‚Äô aggregator doesn‚Äôt need to rollover your indices.\nThe last step and obligatory to trigger all the rollover processes inside Elasticsearch, it creates the first rollover index according to the template and aliases defined inside this.\n1 2 3 4 5 6 7 8 9 10 # Create the first rollover manually (it is necessary) # to trigger ISM Policy association PUT sample-logs-rollover-000001 { \u0026#34;aliases\u0026#34;: { \u0026#34;sample-logs-rollover\u0026#34;:{ \u0026#34;is_write_index\u0026#34;: true } } } So, How do I index my data now?\nUsing the rollover alias (template definition above line 5) created in the Elasticsearch template. Now you have only one index name (index alias) to configure your Custom Program / LogStash / Fluentd, etc and you can forget the suffix pattern.\nHere is an example of how to insert data using the rollover index alias:\n1 2 3 4 5 6 # Bulk load sample, NOTE: To insert data use the rollover aliases POST _bulk {\u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;sample-logs-rollover\u0026#34;}} {\u0026#34;message\u0026#34;: \u0026#34;This is a log sample 1\u0026#34;, \u0026#34;@timestamp\u0026#34;: \u0026#34;2020-04-26T11:07:00+0000\u0026#34;} {\u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;sample-logs-rollover\u0026#34;}} {\u0026#34;message\u0026#34;: \u0026#34;This is a log sample 2\u0026#34;, \u0026#34;@timestamp\u0026#34;: \u0026#34;2020-04-26T11:08:00+0000\u0026#34;} Conclusions If you have Elasticsearch as your logs storage and index platform and you never used before or heard about it:\nIndex State Management (ISM) ‚Üí Open Distro for Elasticsearch Index lifecycle management (ILM) ‚Üí Elasticsearch Elastic Then, Go fast and learn how to apply this to improve your everyday job.\nAcknowledgements This was possible thanks to my friend Alejandro Sabater, who took his free time to review it and share its recommendation with me.\n","permalink":"https://slashdevops.com/post/2020/05/13/1/managing-the-lifecycle-of-your-elasticsearch-indices/","summary":"Managing the Lifecycle of your Elasticsearch Indices Just like me, you are probably storing your [Applications | Infrastructure | IoT ] Logs / Traces (as a time series) into Elasticsearch or at least considering doing it.\nIf that is the case, you might be wondering how to efficiently manage index lifecycles in an automated and clean manner, then this post is for you!\nWhat\u0026rsquo;s happening? Basically, this means that your log management/aggregator applications are storing the logs in Elasticsearch using the timestamp (of capture, processing, or another one) for every record of data and grouping, using a pattern for every group.","title":"Managing the Lifecycle of your Elasticsearch Indices"}]